{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95c48d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Random Seed:  9432\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import glob, os\n",
    "\n",
    "import pydicom\n",
    "import random \n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# https://scikit-survival.readthedocs.io/en/stable/api/generated/sksurv.metrics.concordance_index_censored.html#\n",
    "from sksurv.metrics import concordance_index_censored, cumulative_dynamic_auc, integrated_brier_score\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "from skimage import filters\n",
    "from skimage import io\n",
    "from skimage import transform \n",
    "\n",
    "from einops import rearrange#, reduce, repeat\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "sys.path.append(\"torchdiffeq-master\") # go to parent dir'\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "#from torchdiffeq import odeint\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "######################## configure device ###############\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\" # 1 is GT 740\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# verify that gpu is recognized\n",
    "print(device)\n",
    "\n",
    "\n",
    "\n",
    "################## Set random seem for reproducibility ##########\n",
    "manualSeed = 9432\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "\n",
    "########## interactive mode for plots ###################\n",
    "plt.ion()   \n",
    "%matplotlib inline\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d033d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd=os.getcwd()\n",
    "sys.path.append(pwd+'/backprop/')\n",
    "\n",
    "from mtadam import MTAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7116f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('run', pwd+\"/backprop/losses.ipynb\")\n",
    "get_ipython().run_line_magic('run', pwd+\"/model/Encoder.ipynb\")\n",
    "get_ipython().run_line_magic('run', pwd+\"/model/Neural_ODE.ipynb\")\n",
    "get_ipython().run_line_magic('run', pwd+\"/backprop/losses.ipynb\")\n",
    "get_ipython().run_line_magic('run', pwd+\"/Dataloader/Dataloader.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417baf81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f581bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_inference(val_loader):\n",
    "    encoder_model.eval()\n",
    "    classifier_model.eval()\n",
    "    ode_func.eval()\n",
    "    \n",
    "    t_inp=torch.from_numpy(np.array([0.0, 6.0, 12.0, 18.0, 24.0, 30.0, 36.0])).to(device)\n",
    "    t_inp=(t_inp/36.0)\n",
    "    \n",
    "    gt_lst=[] # 2D array:  sample, 6 time-points\n",
    "    pred_lst=[]\n",
    "    risk_scr_lst=[]\n",
    "    tcnv_lst=[]\n",
    "    indctr_lst=[]\n",
    "    nm_lst=[]\n",
    "    \n",
    "    for i, sample in enumerate(val_loader):\n",
    "        img=sample['img'].to(device)\n",
    "        gt=sample['gt'] # 6-D  conversion within 0/6/12/18/24/30/36 month time-points\n",
    "        tcnv=sample['tcnv']\n",
    "        indctr=sample['indctr']\n",
    "        nm=sample['nm']\n",
    "        \n",
    "        img=rearrange(img, 'b 1 c h w -> b c h w')\n",
    "        \n",
    "        ### Forward Pass\n",
    "        with torch.no_grad():\n",
    "            ftr=encoder_model(img)         \n",
    "            rsk_curr=classifier_model(ftr)\n",
    "            \n",
    "        ##### ODE to get future risk predictions #####\n",
    "        ftr_ode=odeint(ode_func, ftr, t_inp, \n",
    "                          atol=1e-8, rtol=1e-8, method=ode_solver, options=dict(step_size=stp_sz)) # ftr_ode: t=7,B,768\n",
    "        \n",
    "        ftr_ode=ftr_ode[1:,:,:] # remove t=0    t=6,B,768\n",
    "        B=ftr_ode.shape[1]\n",
    "        #print(B)\n",
    "        #print('ftr_ode: '+str(ftr_ode.shape))\n",
    "        ftr_ode=rearrange(ftr_ode, 't b d -> (b t) d') # (B*6), 768\n",
    "        rsk_ode=classifier_model(ftr_ode) # (B*6,1)\n",
    "        #print('ftr_ode: '+str(ftr_ode.shape))\n",
    "        #print('rsk_ode: '+str(rsk_ode.shape))\n",
    "        \n",
    "        rsk_ode=rearrange(rsk_ode, '(b t) 1 -> b t', b=B, t=6) # B,6 \n",
    "        \n",
    "        p=F.sigmoid(rsk_ode) # B,6\n",
    "        p=p.detach().cpu().numpy()\n",
    "        ################################################\n",
    "        \n",
    "        pred_lst.append(p)\n",
    "        risk_scr_lst.append(rsk_curr.detach().cpu().numpy()) # risk predicted for the current input scan\n",
    "        nm_lst.append(nm)\n",
    "        gt_lst.append(gt)\n",
    "        tcnv_lst.append(tcnv)\n",
    "        indctr_lst.append(indctr)\n",
    "        \n",
    "        del img, gt, tcnv, indctr, nm, ftr,rsk_curr,ftr_ode, rsk_ode, p\n",
    "        \n",
    "    encoder_model.train()\n",
    "    classifier_model.train()\n",
    "    ode_func.train()\n",
    "    \n",
    "    \n",
    "    pred_lst=np.concatenate(pred_lst, axis=0) # or stack?  # B,6\n",
    "    risk_scr_lst=np.concatenate(risk_scr_lst, axis=0)      # B,1\n",
    "    nm_lst=np.concatenate(nm_lst, axis=0)                  # B,\n",
    "    gt_lst=np.concatenate(gt_lst, axis=0)                  # B,6\n",
    "    tcnv_lst=np.concatenate(tcnv_lst, axis=0)              # B,\n",
    "    indctr_lst=np.concatenate(indctr_lst, axis=0)          # B,\n",
    "    \n",
    "    ### Sort the list which is used to define the index for the samples of each bootstrap re-sampling.\n",
    "    idx=np.argsort(nm_lst, axis=0)\n",
    "    pred_lst=pred_lst[idx,:]           # B,6\n",
    "    risk_scr_lst=risk_scr_lst[idx,:]   # B,1\n",
    "    nm_lst=nm_lst[idx]                 # B,\n",
    "    gt_lst=gt_lst[idx,:]               # B,6\n",
    "    tcnv_lst=tcnv_lst[idx]             # B,\n",
    "    indctr_lst=indctr_lst[idx]         # B,\n",
    "    del idx\n",
    "    \n",
    "    ### Check that the sorted nm_lst is same as the validation dataloader\n",
    "    flag=np.array_equal(nm_lst, val_data.nm_lst) # this is the order used to define bootstrap samplings\n",
    "    if flag==False:\n",
    "        print('the nm_lst doesnot match !')\n",
    "    \n",
    "    \n",
    "    ###### Now everything is sorted by name. So now, we can use the pre-saved indices ###\n",
    "    indices=val_data.sampling_index\n",
    "    c_lst=[]\n",
    "    auc_lst=[]\n",
    "    for k in range(0, len(indices)): # No. of bootstrap re-samplings\n",
    "        idx=indices[k]\n",
    "        tmp_rsk=np.squeeze(risk_scr_lst[idx,:], axis=1) # B,\n",
    "        tmp_tcnv=tcnv_lst[idx]\n",
    "        tmp_indctr=indctr_lst[idx]\n",
    "        c_index = concordance_index_censored(tmp_indctr.astype(bool), tmp_tcnv, tmp_rsk)\n",
    "        c_index = c_index[0]\n",
    "        c_lst.append(c_index)\n",
    "        del c_index, tmp_indctr, tmp_tcnv, tmp_rsk\n",
    "        \n",
    "        tmp_gt=gt_lst[idx,:]\n",
    "        tmp_pred=pred_lst[idx,:]\n",
    "        auc=[]\n",
    "        for cls in range(0,6):\n",
    "            gt=tmp_gt[:, cls]\n",
    "            pred=tmp_pred[:, cls]\n",
    "            idx2=np.where(gt !=-1) # -1 implies GT is unavailable (eg. time after censoring has occured)\n",
    "            gt=gt[idx2]\n",
    "            pred=pred[idx2]\n",
    "            # roc_auc_score(y_true, y_score\n",
    "            auc.append(roc_auc_score(gt, pred))\n",
    "            del gt, pred, idx2\n",
    "            # suppose within timepoint 36 months but image is censored at 24 months. then lbl is -1, needs to be avoided\n",
    "        \n",
    "        auc=np.expand_dims(np.array(auc), axis=0)\n",
    "        auc_lst.append(auc) # list of 6-dim arrays\n",
    "        del auc, idx\n",
    "        \n",
    "    \n",
    "    auc_lst=np.concatenate(auc_lst, axis=0) # B,6\n",
    "    c_lst=np.array(c_lst)\n",
    "    \n",
    "    mean_concordance=np.mean(c_lst)\n",
    "    avg_auc=np.mean(auc_lst, axis=1) # avg across 6 time-points\n",
    "    mn_avg_auc=np.mean(avg_auc, axis=0) # avg across each sampling.\n",
    "    # confidence intervals np.percentile(c_lst, 0.95)   and 0.05\n",
    "    \n",
    "    print('\\n CI: '+str(mean_concordance)+'  AUC: '+str(mn_avg_auc))\n",
    "    metric=mean_concordance+mn_avg_auc # this has to be maximized\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13907d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthe use of MTAdam is as simple as using Adam, and requires the following steps: \\n(a) initiating the MTAdam optimizer (in a similar way to Adam). \\n(b) keeping the multi-term loss objective decomposed as a sequence of single terms, \\n    and sending the sequence as an argument to MTAdam.step(). \\n(c) avoid calling the function loss.backward(), since it is done internally in MTAdam.step().\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_one_batch(sample, optimizer, scheduler):\n",
    "    \n",
    "    img1=sample['img1'].to(device)   # B,3,224,224\n",
    "    img2=sample['img2'].to(device)   # B,3,224,224\n",
    "    \n",
    "    img1=rearrange(img1, '1 b 1 c h w -> b c h w')\n",
    "    img2=rearrange(img2, '1 b 1 c h w -> b c h w')\n",
    "    \n",
    "    gt1=torch.unsqueeze(torch.squeeze(sample['gt1'], dim=0), dim=1).to(device)   # B,1\n",
    "    gt2=torch.unsqueeze(torch.squeeze(sample['gt2'], dim=0), dim=1).to(device) \n",
    "    \n",
    "    tcnv1=torch.unsqueeze(torch.squeeze(sample['tcnv1'], dim=0), dim=1).to(device)  # B,1\n",
    "    tcnv2=torch.unsqueeze(torch.squeeze(sample['tcnv2'], dim=0), dim=1).to(device) \n",
    "    \n",
    "    indctr1=torch.unsqueeze(torch.squeeze(sample['indctr1'], dim=0), dim=1).to(device)  # B,1\n",
    "    indctr2=torch.unsqueeze(torch.squeeze(sample['indctr2'], dim=0), dim=1).to(device) \n",
    "    \n",
    "    tintrvl=torch.unsqueeze(torch.squeeze(sample['tintrvl'], dim=0), dim=1).to(device)  # B,1\n",
    "    \n",
    "    ###############################################################################################    \n",
    "    img=torch.cat((img1, img2), dim=0)         # 2B,3,H,W\n",
    "    gt=torch.cat((gt1, gt2), dim=0)            # 2B,1\n",
    "    indctr=torch.cat((indctr1, indctr2), dim=0)# 2B,1\n",
    "    tcnv=torch.cat((tcnv1, tcnv2), dim=0)      # 2B,1    \n",
    "    \n",
    "    #print('img: '+str(img.shape))\n",
    "    #print('gt: '+str(gt.shape))\n",
    "    #print('indctr: '+str(indctr.shape))\n",
    "    #print('tcnv: '+str(tcnv.shape))\n",
    "    ######################################## FORWARD PASS #########################################\n",
    "    B=img.shape[0]//2\n",
    "    ftr=encoder_model(img)         # 2B, 768\n",
    "    rsk=classifier_model(ftr)\n",
    "    \n",
    "    ftr1=ftr[0:B,:] \n",
    "    ftr2=ftr[B:,:]\n",
    "    \n",
    "    #print('ftr: '+str(ftr.shape)+'  ftr1: '+str(ftr1.shape)+'  ftr2: '+str(ftr2.shape))\n",
    "    #print('rsk: '+str(rsk.shape))\n",
    "    \n",
    "    ###############################################################################################\n",
    "    \n",
    "    t_inp, idx=torch.unique(tintrvl, sorted=True, return_inverse=True, return_counts=False, dim=0)\n",
    "    t_inp=t_inp.to(device)\n",
    "    \n",
    "    idx=torch.unsqueeze(idx, dim=1) # B,1\n",
    "    idx_ftr=idx.repeat(1,768) # B,768\n",
    "    idx_ftr=torch.unsqueeze(idx_ftr, dim=0) # 1,B,768\n",
    "    \n",
    "    \n",
    "    ### add t=0 to t_inp\n",
    "    t_inp=torch.squeeze(t_inp, dim=1) \n",
    "    t_inp=torch.cat((torch.from_numpy(np.array([0.0])).to(device), t_inp), dim=0)\n",
    "    \n",
    "    ftr_ode=odeint(ode_func, ftr1, t_inp, \n",
    "                          atol=1e-8, rtol=1e-8, method=ode_solver, options=dict(step_size=stp_sz))\n",
    "    \n",
    "    ### Remove the first t=0 time-point\n",
    "    ftr_ode=ftr_ode[1:,:,:] # 12,12,768\n",
    "    ftr_ode=torch.squeeze(torch.gather(input=ftr_ode, dim=0, index=idx_ftr), dim=0) # B,768\n",
    "    \n",
    "    ###############################################################################################\n",
    "    \n",
    "    rsk_ode=classifier_model(ftr_ode)\n",
    "    #print('ftr_ode: '+str(ftr_ode.shape)+'  rsk_ode: '+str(rsk_ode.shape))\n",
    "    \n",
    "    \n",
    "    ####################################### COMPUTE LOSSES ########################################\n",
    "    # the rsk uses loss with a weight of 3 for + class.\n",
    "    cls_loss=bce_loss_logits(rsk, gt)\n",
    "    \n",
    "    ##################### Consistency (ODE) losses ###########\n",
    "    ftr_ode_loss=F.mse_loss(ftr_ode, ftr2, reduction='mean')\n",
    "    # with logits still required the GT to be [0,1]\n",
    "    rsk_ode_loss=F.binary_cross_entropy_with_logits(rsk_ode, F.sigmoid(rsk[B:,:]), reduction='mean')+ F.binary_cross_entropy_with_logits(rsk_ode, gt[B:,:])\n",
    "    \n",
    "    ###################### Risk score ranking Loss ###########\n",
    "    rank_loss=my_risk_concordance_loss(torch.cat((rsk, rsk_ode), dim=0),\n",
    "                                       torch.cat((indctr, indctr2), dim=0),\n",
    "                                       torch.cat((tcnv, tcnv2), dim=0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################# Backpropagation ###########################\n",
    "    # remove previously stored gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Compute Gradients\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step([rank_loss, rsk_ode_loss, cls_loss, ftr_ode_loss], [1, 1, 1, 1], None)\n",
    "    #optimizer.step()\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    ############  Return Loss for Displaying #####\n",
    "    cls_loss=cls_loss.detach().cpu().numpy()\n",
    "    ftr_ode_loss=ftr_ode_loss.detach().cpu().numpy()\n",
    "    rsk_ode_loss=rsk_ode_loss.detach().cpu().numpy()\n",
    "    rank_loss=rank_loss.detach().cpu().numpy()\n",
    "    \n",
    "    loss=cls_loss+ftr_ode_loss+rsk_ode_loss+rank_loss\n",
    "    \n",
    "    return loss, cls_loss, ftr_ode_loss, rsk_ode_loss, rank_loss, optimizer, scheduler\n",
    "     \n",
    "'''\n",
    "the use of MTAdam is as simple as using Adam, and requires the following steps: \n",
    "(a) initiating the MTAdam optimizer (in a similar way to Adam). \n",
    "(b) keeping the multi-term loss objective decomposed as a sequence of single terms, \n",
    "    and sending the sequence as an argument to MTAdam.step(). \n",
    "(c) avoid calling the function loss.backward(), since it is done internally in MTAdam.step().\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1114ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_complete():\n",
    "    mn_lr=10**(-6.0)\n",
    "    mx_lr=10**(-4.0)\n",
    "    \n",
    "    nupdates=300#1000  200 # no of batch updates in each epoch\n",
    "    max_epochs=200\n",
    "    max_patience=50 # 100 # Early stopping if validation metric doesnot improve in this many consecutive epochs\n",
    "    \n",
    "    ####\n",
    "    max_metric=0\n",
    "    \n",
    "    ############################################################################################################\n",
    "    optimizer = MTAdam(list(encoder_model.parameters()) + list(classifier_model.parameters()) + list(ode_func.parameters()) + list(rank_model.parameters()), lr=mn_lr, amsgrad=True)\n",
    "    scheduler=torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=mn_lr, max_lr=mx_lr, cycle_momentum=False,\n",
    "                                            step_size_up=nupdates//2, step_size_down=None, mode='triangular')\n",
    "    ############################################################################################################\n",
    "    \n",
    "    patience=0\n",
    "    data_iter = iter(train_loader) \n",
    "    for epochs in range(0, max_epochs):\n",
    "        run_loss=0 # total loss  \n",
    "        run_cls_loss=0 #  BCE loss(mixup) with GT (both Encoder and ODE)\n",
    "        run_ftr_ode_loss=0 # MSE loss of ODE feature pred\n",
    "        run_rsk_ode_loss=0  # BCE loss of consistency in pred for ODE\n",
    "        run_rnk_loss=0  # Concordance Index loss\n",
    "        \n",
    "        \n",
    "        tic=time.time()\n",
    "        for i in range(0, nupdates): # batch updates in each round of training (epoch) \n",
    "            try:\n",
    "                sample = next(data_iter) \n",
    "            except StopIteration:\n",
    "                # StopIteration is thrown if dataset ends\n",
    "                # reinitialize data loader \n",
    "                data_iter = iter(train_loader)\n",
    "                sample = next(data_iter)\n",
    "                \n",
    "            \n",
    "            \n",
    "            loss, cls_loss, ftr_ode_loss, rsk_ode_loss, rank_loss, optimizer, scheduler=train_one_batch(sample, optimizer, scheduler)\n",
    "            del sample\n",
    "            \n",
    "                \n",
    "            run_loss=run_loss+loss\n",
    "            run_cls_loss=run_cls_loss+cls_loss\n",
    "            run_ftr_ode_loss=run_ftr_ode_loss+ftr_ode_loss\n",
    "            \n",
    "            run_rsk_ode_loss=run_rsk_ode_loss+rsk_ode_loss\n",
    "            run_rnk_loss=run_rnk_loss+rank_loss\n",
    "            \n",
    "            del loss, cls_loss, ftr_ode_loss, rsk_ode_loss, rank_loss\n",
    "                \n",
    "            if (i+1) % 10== 0: # displays after every 10 batch updates\n",
    "                print (\"Epoch [{}/{}], Batch [{}/{}], Train Loss: {:.4f}, Classification: {:.4f}, FTR_ODE: {:.4f}, RSK_ODE: {:.4f}, RANKING: {:.4f}\"\n",
    "                       .format(epochs+1, max_epochs, i+1, nupdates, (run_loss/i), (run_cls_loss/i), (run_ftr_ode_loss/i), (run_rsk_ode_loss/i), (run_rnk_loss/i)), end =\"\\r\")\n",
    "        \n",
    "            \n",
    "        ### End of an epoch. Check validation loss\n",
    "        metric=complete_inference(val_loader) \n",
    "        toc=time.time()\n",
    "        print('\\n Val Metric: '+str(metric)+'  Last Epoch took '+str(toc-tic)+' seconds')\n",
    "        \n",
    "        \n",
    "        run_loss=0 # total loss  \n",
    "        run_cls_loss=0 #  BCE loss(mixup) with GT (both Encoder and ODE)\n",
    "        run_ftr_ode_loss=0 # MSE loss of ODE feature pred\n",
    "        run_r_ode_loss=0  # BCE loss of consistency in pred for ODE\n",
    "        run_risk_loss=0  # Concordance Index loss\n",
    "        \n",
    "        #### Early stopping\n",
    "        if metric>max_metric:\n",
    "            max_metric=metric\n",
    "            patience=0\n",
    "            print('Validation metric improved !')\n",
    "            torch.save({\n",
    "                        'ode_state_dict': ode_func.state_dict(),\n",
    "                        'encoder_state_dict': encoder_model.state_dict(),\n",
    "                        'rank_state_dict': rank_model.state_dict(),\n",
    "                        'classifier_state_dict': classifier_model.state_dict()\n",
    "            },'best_weight_fld'+str(fld)+'_metric'+str(metric)+'.pt')\n",
    "        else:\n",
    "            patience=patience+1\n",
    "            print('\\n Validation metric has not improved in last '+str(patience)+' epochs')\n",
    "            if patience>max_patience:\n",
    "                print('Early Stopping !')\n",
    "                break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728f4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e4bd59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 X 2 samples:   \n",
    "# 6 X 2 non-converters + 6 converters(first image has not converted)=18 non converters\n",
    "# 6 converters.\n",
    "bce_loss_logits=nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([3.0]).to(device))\n",
    "\n",
    "\n",
    "# for ODE: 6 converter + 6 non-converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62273bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607\n",
      "Epoch [1/150], Batch [200/200], Train Loss: 2.2978, Classification: 0.7626, FTR_ODE: 0.0169, RSK_ODE: 1.1687, RANKING: 0.3497\n",
      " CI: 0.7457176243690165  AUC: 0.7971576391228378\n",
      "\n",
      " Val Metric: 1.542875263491854  Last Epoch took 722.8822522163391 seconds\n",
      "Validation metric improved !\n",
      "Epoch [2/150], Batch [200/200], Train Loss: 1.5130, Classification: 0.4648, FTR_ODE: 0.0107, RSK_ODE: 0.8476, RANKING: 0.1899\n",
      " CI: 0.7458335205721834  AUC: 0.7950954196304759\n",
      "\n",
      " Val Metric: 1.5409289402026594  Last Epoch took 806.8543956279755 seconds\n",
      "\n",
      " Validation metric has not improved in last 1 epochs\n",
      "Epoch [3/150], Batch [200/200], Train Loss: 1.2183, Classification: 0.3636, FTR_ODE: 0.0090, RSK_ODE: 0.7010, RANKING: 0.1448\n",
      " CI: 0.7500369592649101  AUC: 0.8072814229731856\n",
      "\n",
      " Val Metric: 1.5573183822380958  Last Epoch took 812.2652823925018 seconds\n",
      "Validation metric improved !\n",
      "Epoch [4/150], Batch [200/200], Train Loss: 0.9961, Classification: 0.2755, FTR_ODE: 0.0072, RSK_ODE: 0.6000, RANKING: 0.1133\n",
      " CI: 0.7629868255276884  AUC: 0.8267127572607499\n",
      "\n",
      " Val Metric: 1.5896995827884384  Last Epoch took 800.23064661026 seconds\n",
      "Validation metric improved !\n",
      "Epoch [5/150], Batch [200/200], Train Loss: 0.8807, Classification: 0.2309, FTR_ODE: 0.0062, RSK_ODE: 0.5413, RANKING: 0.1023\n",
      " CI: 0.7509939700715654  AUC: 0.8041050358506511\n",
      "\n",
      " Val Metric: 1.5550990059222165  Last Epoch took 793.2026419639587 seconds\n",
      "\n",
      " Validation metric has not improved in last 1 epochs\n",
      "Epoch [6/150], Batch [200/200], Train Loss: 0.7785, Classification: 0.2000, FTR_ODE: 0.0052, RSK_ODE: 0.4826, RANKING: 0.0907\n",
      " CI: 0.7350052877484955  AUC: 0.791228777124897\n",
      "\n",
      " Val Metric: 1.5262340648733925  Last Epoch took 810.751095533371 seconds\n",
      "\n",
      " Validation metric has not improved in last 2 epochs\n",
      "Epoch [7/150], Batch [200/200], Train Loss: 0.6359, Classification: 0.1424, FTR_ODE: 0.0045, RSK_ODE: 0.4158, RANKING: 0.0732\n",
      " CI: 0.7318178345758732  AUC: 0.7768583143262118\n",
      "\n",
      " Val Metric: 1.508676148902085  Last Epoch took 813.0772449970245 seconds\n",
      "\n",
      " Validation metric has not improved in last 3 epochs\n",
      "Epoch [8/150], Batch [200/200], Train Loss: 0.6477, Classification: 0.1517, FTR_ODE: 0.0041, RSK_ODE: 0.4180, RANKING: 0.0739\n",
      " CI: 0.7149162920533545  AUC: 0.7582188165443733\n",
      "\n",
      " Val Metric: 1.4731351085977278  Last Epoch took 808.7289736270905 seconds\n",
      "\n",
      " Validation metric has not improved in last 4 epochs\n",
      "Epoch [9/150], Batch [200/200], Train Loss: 0.5737, Classification: 0.1189, FTR_ODE: 0.0040, RSK_ODE: 0.3844, RANKING: 0.0663\n",
      " CI: 0.7199744231839468  AUC: 0.7762435217525281\n",
      "\n",
      " Val Metric: 1.496217944936475  Last Epoch took 815.4436340332031 seconds\n",
      "\n",
      " Validation metric has not improved in last 5 epochs\n",
      "Epoch [10/150], Batch [200/200], Train Loss: 0.5860, Classification: 0.1387, FTR_ODE: 0.0035, RSK_ODE: 0.3793, RANKING: 0.0645\n",
      " CI: 0.7270819407348269  AUC: 0.7843694360388397\n",
      "\n",
      " Val Metric: 1.5114513767736666  Last Epoch took 814.4714260101318 seconds\n",
      "\n",
      " Validation metric has not improved in last 6 epochs\n",
      "Epoch [11/150], Batch [200/200], Train Loss: 0.6326, Classification: 0.1312, FTR_ODE: 0.0031, RSK_ODE: 0.4250, RANKING: 0.0732\n",
      " CI: 0.767348854200742  AUC: 0.8206626234358492\n",
      "\n",
      " Val Metric: 1.588011477636591  Last Epoch took 800.7339925765991 seconds\n",
      "\n",
      " Validation metric has not improved in last 7 epochs\n",
      "Epoch [12/150], Batch [200/200], Train Loss: 0.5954, Classification: 0.1281, FTR_ODE: 0.0027, RSK_ODE: 0.3937, RANKING: 0.0707\n",
      " CI: 0.7177944635427808  AUC: 0.7624512668007176\n",
      "\n",
      " Val Metric: 1.4802457303434984  Last Epoch took 789.6608319282532 seconds\n",
      "\n",
      " Validation metric has not improved in last 8 epochs\n",
      "Epoch [13/150], Batch [200/200], Train Loss: 0.4563, Classification: 0.0932, FTR_ODE: 0.0028, RSK_ODE: 0.3035, RANKING: 0.0567\n",
      " CI: 0.7693342132493324  AUC: 0.8235338422692874\n",
      "\n",
      " Val Metric: 1.5928680555186197  Last Epoch took 784.9698779582977 seconds\n",
      "Validation metric improved !\n",
      "Epoch [14/150], Batch [200/200], Train Loss: 0.5274, Classification: 0.0926, FTR_ODE: 0.0025, RSK_ODE: 0.3719, RANKING: 0.0604\n",
      " CI: 0.7283230400795485  AUC: 0.7727379007903387\n",
      "\n",
      " Val Metric: 1.5010609408698872  Last Epoch took 792.1664369106293 seconds\n",
      "\n",
      " Validation metric has not improved in last 1 epochs\n",
      "Epoch [15/150], Batch [200/200], Train Loss: 0.4376, Classification: 0.0755, FTR_ODE: 0.0025, RSK_ODE: 0.3072, RANKING: 0.0524\n",
      " CI: 0.7202549941647847  AUC: 0.7644799178364167\n",
      "\n",
      " Val Metric: 1.4847349120012012  Last Epoch took 813.0713725090027 seconds\n",
      "\n",
      " Validation metric has not improved in last 2 epochs\n",
      "Epoch [16/150], Batch [200/200], Train Loss: 0.4367, Classification: 0.0933, FTR_ODE: 0.0022, RSK_ODE: 0.2921, RANKING: 0.0491\n",
      " CI: 0.7185861018713119  AUC: 0.7688417496127129\n",
      "\n",
      " Val Metric: 1.487427851484025  Last Epoch took 813.5388851165771 seconds\n",
      "\n",
      " Validation metric has not improved in last 3 epochs\n",
      "Epoch [17/150], Batch [200/200], Train Loss: 0.4151, Classification: 0.0657, FTR_ODE: 0.0022, RSK_ODE: 0.2973, RANKING: 0.0499\n",
      " CI: 0.6792032347421482  AUC: 0.7387232110409154\n",
      "\n",
      " Val Metric: 1.4179264457830636  Last Epoch took 804.9935600757599 seconds\n",
      "\n",
      " Validation metric has not improved in last 4 epochs\n",
      "Epoch [18/150], Batch [200/200], Train Loss: 0.3916, Classification: 0.0687, FTR_ODE: 0.0021, RSK_ODE: 0.2740, RANKING: 0.0467\n",
      " CI: 0.7495906891430067  AUC: 0.7861728909961042\n",
      "\n",
      " Val Metric: 1.535763580139111  Last Epoch took 804.8292744159698 seconds\n",
      "\n",
      " Validation metric has not improved in last 5 epochs\n",
      "Epoch [19/150], Batch [200/200], Train Loss: 0.3391, Classification: 0.0584, FTR_ODE: 0.0019, RSK_ODE: 0.2359, RANKING: 0.0429\n",
      " CI: 0.689489912982697  AUC: 0.7435005900947407\n",
      "\n",
      " Val Metric: 1.4329905030774377  Last Epoch took 804.3941822052002 seconds\n",
      "\n",
      " Validation metric has not improved in last 6 epochs\n",
      "Epoch [20/150], Batch [200/200], Train Loss: 0.3680, Classification: 0.0553, FTR_ODE: 0.0021, RSK_ODE: 0.2655, RANKING: 0.0450\n",
      " CI: 0.6965869828405157  AUC: 0.7430112573287784\n",
      "\n",
      " Val Metric: 1.439598240169294  Last Epoch took 803.8499238491058 seconds\n",
      "\n",
      " Validation metric has not improved in last 7 epochs\n",
      "Epoch [21/150], Batch [200/200], Train Loss: 0.2879, Classification: 0.0320, FTR_ODE: 0.0020, RSK_ODE: 0.2162, RANKING: 0.0376\n",
      " CI: 0.7263708997100246  AUC: 0.774448733985851\n",
      "\n",
      " Val Metric: 1.5008196336958757  Last Epoch took 802.9159832000732 seconds\n",
      "\n",
      " Validation metric has not improved in last 8 epochs\n",
      "Epoch [22/150], Batch [200/200], Train Loss: 0.5017, Classification: 0.1172, FTR_ODE: 0.0021, RSK_ODE: 0.3296, RANKING: 0.0528\n",
      " CI: 0.7321962034586172  AUC: 0.7838549032464381\n",
      "\n",
      " Val Metric: 1.5160511067050555  Last Epoch took 811.3743851184845 seconds\n",
      "\n",
      " Validation metric has not improved in last 9 epochs\n",
      "Epoch [23/150], Batch [200/200], Train Loss: 0.3893, Classification: 0.0492, FTR_ODE: 0.0018, RSK_ODE: 0.2941, RANKING: 0.0442\n",
      " CI: 0.7181117615546186  AUC: 0.7753868870575437\n",
      "\n",
      " Val Metric: 1.4934986486121624  Last Epoch took 805.734623670578 seconds\n",
      "\n",
      " Validation metric has not improved in last 10 epochs\n",
      "Epoch [24/150], Batch [200/200], Train Loss: 0.3563, Classification: 0.0547, FTR_ODE: 0.0018, RSK_ODE: 0.2585, RANKING: 0.0414\n",
      " CI: 0.7255614708354544  AUC: 0.7769948328461315\n",
      "\n",
      " Val Metric: 1.5025563036815859  Last Epoch took 814.0943920612335 seconds\n",
      "\n",
      " Validation metric has not improved in last 11 epochs\n",
      "Epoch [25/150], Batch [200/200], Train Loss: 0.3788, Classification: 0.0633, FTR_ODE: 0.0017, RSK_ODE: 0.2727, RANKING: 0.0412\n",
      " CI: 0.7167198443121192  AUC: 0.7624643814102733\n",
      "\n",
      " Val Metric: 1.4791842257223924  Last Epoch took 815.3580362796783 seconds\n",
      "\n",
      " Validation metric has not improved in last 12 epochs\n",
      "Epoch [26/150], Batch [200/200], Train Loss: 0.2822, Classification: 0.0455, FTR_ODE: 0.0017, RSK_ODE: 0.1979, RANKING: 0.0372\n",
      " CI: 0.676739744321552  AUC: 0.7279242344209766\n",
      "\n",
      " Val Metric: 1.4046639787425286  Last Epoch took 798.9616334438324 seconds\n",
      "\n",
      " Validation metric has not improved in last 13 epochs\n",
      "Epoch [27/150], Batch [200/200], Train Loss: 0.3265, Classification: 0.0529, FTR_ODE: 0.0018, RSK_ODE: 0.2358, RANKING: 0.0360\n",
      " CI: 0.7321588370955348  AUC: 0.7885110017615652\n",
      "\n",
      " Val Metric: 1.5206698388571  Last Epoch took 797.1605868339539 seconds\n",
      "\n",
      " Validation metric has not improved in last 14 epochs\n",
      "Epoch [28/150], Batch [200/200], Train Loss: 0.2701, Classification: 0.0343, FTR_ODE: 0.0018, RSK_ODE: 0.2008, RANKING: 0.0333\n",
      " CI: 0.7277307923385588  AUC: 0.7888512683018217\n",
      "\n",
      " Val Metric: 1.5165820606403804  Last Epoch took 809.5832979679108 seconds\n",
      "\n",
      " Validation metric has not improved in last 15 epochs\n",
      "Epoch [29/150], Batch [200/200], Train Loss: 0.2923, Classification: 0.0393, FTR_ODE: 0.0018, RSK_ODE: 0.2160, RANKING: 0.0353\n",
      " CI: 0.7433618243248283  AUC: 0.8131094985916144\n",
      "\n",
      " Val Metric: 1.5564713229164426  Last Epoch took 813.2206783294678 seconds\n",
      "\n",
      " Validation metric has not improved in last 16 epochs\n",
      "Epoch [30/150], Batch [200/200], Train Loss: 0.3165, Classification: 0.0417, FTR_ODE: 0.0016, RSK_ODE: 0.2357, RANKING: 0.0375\n",
      " CI: 0.7325325144629664  AUC: 0.8045713434665287\n",
      "\n",
      " Val Metric: 1.537103857929495  Last Epoch took 808.9372758865356 seconds\n",
      "\n",
      " Validation metric has not improved in last 17 epochs\n",
      "Epoch [31/150], Batch [200/200], Train Loss: 0.3025, Classification: 0.0408, FTR_ODE: 0.0016, RSK_ODE: 0.2221, RANKING: 0.0380\n",
      " CI: 0.6792106302062639  AUC: 0.740878514592077\n",
      "\n",
      " Val Metric: 1.4200891447983408  Last Epoch took 806.6586232185364 seconds\n",
      "\n",
      " Validation metric has not improved in last 18 epochs\n",
      "Epoch [32/150], Batch [200/200], Train Loss: 0.2661, Classification: 0.0402, FTR_ODE: 0.0014, RSK_ODE: 0.1918, RANKING: 0.0327\n",
      " CI: 0.6852106516148752  AUC: 0.7548919341289948\n",
      "\n",
      " Val Metric: 1.4401025857438698  Last Epoch took 814.5787587165833 seconds\n",
      "\n",
      " Validation metric has not improved in last 19 epochs\n",
      "Epoch [33/150], Batch [200/200], Train Loss: 0.3428, Classification: 0.0509, FTR_ODE: 0.0016, RSK_ODE: 0.2529, RANKING: 0.0374\n",
      " CI: 0.7258521445819152  AUC: 0.7895329233867769\n",
      "\n",
      " Val Metric: 1.5153850679686922  Last Epoch took 815.7933027744293 seconds\n",
      "\n",
      " Validation metric has not improved in last 20 epochs\n",
      "Epoch [34/150], Batch [200/200], Train Loss: 0.3502, Classification: 0.0516, FTR_ODE: 0.0016, RSK_ODE: 0.2588, RANKING: 0.0382\n",
      " CI: 0.711502112949132  AUC: 0.7618632927385028\n",
      "\n",
      " Val Metric: 1.4733654056876349  Last Epoch took 802.8046386241913 seconds\n",
      "\n",
      " Validation metric has not improved in last 21 epochs\n",
      "Epoch [35/150], Batch [200/200], Train Loss: 0.3102, Classification: 0.0385, FTR_ODE: 0.0016, RSK_ODE: 0.2325, RANKING: 0.0376\n",
      " CI: 0.708870594483169  AUC: 0.767155930905399\n",
      "\n",
      " Val Metric: 1.476026525388568  Last Epoch took 810.4288334846497 seconds\n",
      "\n",
      " Validation metric has not improved in last 22 epochs\n",
      "Epoch [36/150], Batch [200/200], Train Loss: 0.3546, Classification: 0.0675, FTR_ODE: 0.0014, RSK_ODE: 0.2464, RANKING: 0.0393\n",
      " CI: 0.7228683076133148  AUC: 0.7772598591477757\n",
      "\n",
      " Val Metric: 1.5001281667610904  Last Epoch took 811.411598443985 seconds\n",
      "\n",
      " Validation metric has not improved in last 23 epochs\n",
      "Epoch [37/150], Batch [200/200], Train Loss: 0.3440, Classification: 0.0653, FTR_ODE: 0.0012, RSK_ODE: 0.2372, RANKING: 0.0403\n",
      " CI: 0.695740731505977  AUC: 0.7324667722813979\n",
      "\n",
      " Val Metric: 1.428207503787375  Last Epoch took 808.5295712947845 seconds\n",
      "\n",
      " Validation metric has not improved in last 24 epochs\n",
      "Epoch [38/150], Batch [200/200], Train Loss: 0.2823, Classification: 0.0330, FTR_ODE: 0.0013, RSK_ODE: 0.2156, RANKING: 0.0325\n",
      " CI: 0.6919440570139236  AUC: 0.7396916510780046\n",
      "\n",
      " Val Metric: 1.4316357080919282  Last Epoch took 791.6817662715912 seconds\n",
      "\n",
      " Validation metric has not improved in last 25 epochs\n",
      "Epoch [39/150], Batch [200/200], Train Loss: 0.2305, Classification: 0.0221, FTR_ODE: 0.0013, RSK_ODE: 0.1768, RANKING: 0.0303\n",
      " CI: 0.6967002580464676  AUC: 0.7369289766310443\n",
      "\n",
      " Val Metric: 1.433629234677512  Last Epoch took 804.821772813797 seconds\n",
      "\n",
      " Validation metric has not improved in last 26 epochs\n",
      "Epoch [40/150], Batch [200/200], Train Loss: 0.2628, Classification: 0.0181, FTR_ODE: 0.0014, RSK_ODE: 0.2127, RANKING: 0.0306\n",
      " CI: 0.6955351595663704  AUC: 0.7477898777236343\n",
      "\n",
      " Val Metric: 1.4433250372900046  Last Epoch took 800.3877444267273 seconds\n",
      "\n",
      " Validation metric has not improved in last 27 epochs\n",
      "Epoch [41/150], Batch [200/200], Train Loss: 0.2795, Classification: 0.0400, FTR_ODE: 0.0014, RSK_ODE: 0.2060, RANKING: 0.0322\n",
      " CI: 0.705403446500394  AUC: 0.7520390067061191\n",
      "\n",
      " Val Metric: 1.4574424532065131  Last Epoch took 786.1084458827972 seconds\n",
      "\n",
      " Validation metric has not improved in last 28 epochs\n",
      "Epoch [42/150], Batch [200/200], Train Loss: 0.2460, Classification: 0.0358, FTR_ODE: 0.0011, RSK_ODE: 0.1768, RANKING: 0.0323\n",
      " CI: 0.6906935368236725  AUC: 0.7432255747724935\n",
      "\n",
      " Val Metric: 1.433919111596166  Last Epoch took 787.8409445285797 seconds\n",
      "\n",
      " Validation metric has not improved in last 29 epochs\n",
      "Epoch [43/150], Batch [200/200], Train Loss: 0.3263, Classification: 0.0572, FTR_ODE: 0.0013, RSK_ODE: 0.2317, RANKING: 0.0362\n",
      " CI: 0.7165392108766354  AUC: 0.7920230757708131\n",
      "\n",
      " Val Metric: 1.5085622866474484  Last Epoch took 810.9644775390625 seconds\n",
      "\n",
      " Validation metric has not improved in last 30 epochs\n",
      "Epoch [44/150], Batch [200/200], Train Loss: 0.2362, Classification: 0.0352, FTR_ODE: 0.0011, RSK_ODE: 0.1687, RANKING: 0.0312\n",
      " CI: 0.7049689225263825  AUC: 0.767688996257723\n",
      "\n",
      " Val Metric: 1.4726579187841056  Last Epoch took 808.9209780693054 seconds\n",
      "\n",
      " Validation metric has not improved in last 31 epochs\n",
      "Epoch [45/150], Batch [200/200], Train Loss: 0.2123, Classification: 0.0307, FTR_ODE: 0.0011, RSK_ODE: 0.1492, RANKING: 0.0314\n",
      " CI: 0.7116965073336537  AUC: 0.7561171153478224\n",
      "\n",
      " Val Metric: 1.4678136226814762  Last Epoch took 801.8568861484528 seconds\n",
      "\n",
      " Validation metric has not improved in last 32 epochs\n",
      "Epoch [46/150], Batch [200/200], Train Loss: 0.3537, Classification: 0.0600, FTR_ODE: 0.0013, RSK_ODE: 0.2551, RANKING: 0.0374\n",
      " CI: 0.7291548315163102  AUC: 0.7951113844136772\n",
      "\n",
      " Val Metric: 1.5242662159299876  Last Epoch took 806.1983664035797 seconds\n",
      "\n",
      " Validation metric has not improved in last 33 epochs\n",
      "Epoch [47/150], Batch [200/200], Train Loss: 0.2693, Classification: 0.0442, FTR_ODE: 0.0010, RSK_ODE: 0.1911, RANKING: 0.0330\n",
      " CI: 0.7254943759475667  AUC: 0.7870438400723817\n",
      "\n",
      " Val Metric: 1.5125382160199483  Last Epoch took 808.0448064804077 seconds\n",
      "\n",
      " Validation metric has not improved in last 34 epochs\n",
      "Epoch [48/150], Batch [200/200], Train Loss: 0.2489, Classification: 0.0324, FTR_ODE: 0.0010, RSK_ODE: 0.1849, RANKING: 0.0306\n",
      " CI: 0.7269956774075103  AUC: 0.7877111254422646\n",
      "\n",
      " Val Metric: 1.514706802849775  Last Epoch took 800.622035741806 seconds\n",
      "\n",
      " Validation metric has not improved in last 35 epochs\n",
      "Epoch [49/150], Batch [200/200], Train Loss: 0.2769, Classification: 0.0590, FTR_ODE: 0.0011, RSK_ODE: 0.1824, RANKING: 0.0345\n",
      " CI: 0.7191325714915769  AUC: 0.7862304857268891\n",
      "\n",
      " Val Metric: 1.505363057218466  Last Epoch took 810.1983489990234 seconds\n",
      "\n",
      " Validation metric has not improved in last 36 epochs\n",
      "Epoch [50/150], Batch [200/200], Train Loss: 0.5013, Classification: 0.1349, FTR_ODE: 0.0011, RSK_ODE: 0.3190, RANKING: 0.0465\n",
      " CI: 0.6881535244086787  AUC: 0.7500784748152498\n",
      "\n",
      " Val Metric: 1.4382319992239285  Last Epoch took 825.7823469638824 seconds\n",
      "\n",
      " Validation metric has not improved in last 37 epochs\n",
      "Epoch [51/150], Batch [200/200], Train Loss: 0.3392, Classification: 0.0533, FTR_ODE: 0.0009, RSK_ODE: 0.2476, RANKING: 0.0374\n",
      " CI: 0.7238757004477113  AUC: 0.7676124230127042\n",
      "\n",
      " Val Metric: 1.4914881234604156  Last Epoch took 807.5735366344452 seconds\n",
      "\n",
      " Validation metric has not improved in last 38 epochs\n",
      "Epoch [52/150], Batch [200/200], Train Loss: 0.3240, Classification: 0.0681, FTR_ODE: 0.0010, RSK_ODE: 0.2191, RANKING: 0.0359\n",
      " CI: 0.6926941715718633  AUC: 0.7494401204047463\n",
      "\n",
      " Val Metric: 1.4421342919766096  Last Epoch took 815.5660345554352 seconds\n",
      "\n",
      " Validation metric has not improved in last 39 epochs\n",
      "Epoch [53/150], Batch [200/200], Train Loss: 0.2360, Classification: 0.0232, FTR_ODE: 0.0010, RSK_ODE: 0.1826, RANKING: 0.0292\n",
      " CI: 0.6853961594657048  AUC: 0.7319542499653892\n",
      "\n",
      " Val Metric: 1.4173504094310938  Last Epoch took 791.4563677310944 seconds\n",
      "\n",
      " Validation metric has not improved in last 40 epochs\n",
      "Epoch [54/150], Batch [200/200], Train Loss: 0.2815, Classification: 0.0461, FTR_ODE: 0.0009, RSK_ODE: 0.2026, RANKING: 0.0320\n",
      " CI: 0.6768630986877875  AUC: 0.7271001346760699\n",
      "\n",
      " Val Metric: 1.4039632333638574  Last Epoch took 804.1517825126648 seconds\n",
      "\n",
      " Validation metric has not improved in last 41 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/150], Batch [200/200], Train Loss: 0.2974, Classification: 0.0512, FTR_ODE: 0.0010, RSK_ODE: 0.2093, RANKING: 0.0360\n",
      " CI: 0.6945123013036941  AUC: 0.7359945222065628\n",
      "\n",
      " Val Metric: 1.430506823510257  Last Epoch took 801.3268096446991 seconds\n",
      "\n",
      " Validation metric has not improved in last 42 epochs\n",
      "Epoch [56/150], Batch [200/200], Train Loss: 0.2548, Classification: 0.0322, FTR_ODE: 0.0010, RSK_ODE: 0.1901, RANKING: 0.0315\n",
      " CI: 0.7303335376632042  AUC: 0.7644273314334022\n",
      "\n",
      " Val Metric: 1.4947608690966065  Last Epoch took 800.6249837875366 seconds\n",
      "\n",
      " Validation metric has not improved in last 43 epochs\n",
      "Epoch [57/150], Batch [200/200], Train Loss: 0.3276, Classification: 0.0646, FTR_ODE: 0.0010, RSK_ODE: 0.2252, RANKING: 0.0368\n",
      " CI: 0.7096611261121806  AUC: 0.7546389153481909\n",
      "\n",
      " Val Metric: 1.4643000414603715  Last Epoch took 809.5641093254089 seconds\n",
      "\n",
      " Validation metric has not improved in last 44 epochs\n",
      "Epoch [58/150], Batch [200/200], Train Loss: 0.3218, Classification: 0.0453, FTR_ODE: 0.0009, RSK_ODE: 0.2418, RANKING: 0.0339\n",
      " CI: 0.7142363447089007  AUC: 0.7624437269164229\n",
      "\n",
      " Val Metric: 1.4766800716253237  Last Epoch took 815.527425289154 seconds\n",
      "\n",
      " Validation metric has not improved in last 45 epochs\n",
      "Epoch [59/150], Batch [200/200], Train Loss: 0.2977, Classification: 0.0462, FTR_ODE: 0.0009, RSK_ODE: 0.2172, RANKING: 0.0334\n",
      " CI: 0.7091830030056011  AUC: 0.7507538369273824\n",
      "\n",
      " Val Metric: 1.4599368399329835  Last Epoch took 810.2006752490997 seconds\n",
      "\n",
      " Validation metric has not improved in last 46 epochs\n",
      "Epoch [60/150], Batch [200/200], Train Loss: 0.2116, Classification: 0.0263, FTR_ODE: 0.0009, RSK_ODE: 0.1556, RANKING: 0.0288\n",
      " CI: 0.6926258204188238  AUC: 0.7228753161214668\n",
      "\n",
      " Val Metric: 1.4155011365402905  Last Epoch took 812.8871262073517 seconds\n",
      "\n",
      " Validation metric has not improved in last 47 epochs\n",
      "Epoch [61/150], Batch [200/200], Train Loss: 0.3279, Classification: 0.0481, FTR_ODE: 0.0011, RSK_ODE: 0.2458, RANKING: 0.0329\n",
      " CI: 0.6585646383428974  AUC: 0.7019629687413669\n",
      "\n",
      " Val Metric: 1.3605276070842642  Last Epoch took 810.5217900276184 seconds\n",
      "\n",
      " Validation metric has not improved in last 48 epochs\n",
      "Epoch [62/150], Batch [200/200], Train Loss: 0.2623, Classification: 0.0303, FTR_ODE: 0.0008, RSK_ODE: 0.1996, RANKING: 0.0315\n",
      " CI: 0.6774884712904122  AUC: 0.7129213174415908\n",
      "\n",
      " Val Metric: 1.390409788732003  Last Epoch took 807.4584496021271 seconds\n",
      "\n",
      " Validation metric has not improved in last 49 epochs\n",
      "Epoch [63/150], Batch [200/200], Train Loss: 0.2589, Classification: 0.0427, FTR_ODE: 0.0008, RSK_ODE: 0.1850, RANKING: 0.0303\n",
      " CI: 0.6382896204430831  AUC: 0.6638292002483087\n",
      "\n",
      " Val Metric: 1.3021188206913918  Last Epoch took 808.9232347011566 seconds\n",
      "\n",
      " Validation metric has not improved in last 50 epochs\n",
      "Epoch [64/150], Batch [200/200], Train Loss: 0.2168, Classification: 0.0206, FTR_ODE: 0.0008, RSK_ODE: 0.1685, RANKING: 0.0269\n",
      " CI: 0.658095429078101  AUC: 0.690881609540924\n",
      "\n",
      " Val Metric: 1.348977038619025  Last Epoch took 832.9773623943329 seconds\n",
      "\n",
      " Validation metric has not improved in last 51 epochs\n",
      "Epoch [65/150], Batch [200/200], Train Loss: 0.2039, Classification: 0.0213, FTR_ODE: 0.0009, RSK_ODE: 0.1570, RANKING: 0.0246\n",
      " CI: 0.6445233365598817  AUC: 0.661295036414933\n",
      "\n",
      " Val Metric: 1.3058183729748147  Last Epoch took 812.3670573234558 seconds\n",
      "\n",
      " Validation metric has not improved in last 52 epochs\n",
      "Epoch [66/150], Batch [200/200], Train Loss: 0.1776, Classification: 0.0043, FTR_ODE: 0.0010, RSK_ODE: 0.1476, RANKING: 0.0246\n",
      " CI: 0.6388460774556008  AUC: 0.6367480659210994\n",
      "\n",
      " Val Metric: 1.2755941433767002  Last Epoch took 813.8845613002777 seconds\n",
      "\n",
      " Validation metric has not improved in last 53 epochs\n",
      "Epoch [67/150], Batch [200/200], Train Loss: 0.2871, Classification: 0.0232, FTR_ODE: 0.0013, RSK_ODE: 0.2369, RANKING: 0.0257\n",
      " CI: 0.6676602515034857  AUC: 0.6961575000555995\n",
      "\n",
      " Val Metric: 1.3638177515590852  Last Epoch took 803.8830778598785 seconds\n",
      "\n",
      " Validation metric has not improved in last 54 epochs\n",
      "Epoch [68/150], Batch [200/200], Train Loss: 0.2016, Classification: 0.0089, FTR_ODE: 0.0010, RSK_ODE: 0.1647, RANKING: 0.0270\n",
      " CI: 0.6424088604174171  AUC: 0.674973729335545\n",
      "\n",
      " Val Metric: 1.317382589752962  Last Epoch took 802.2061879634857 seconds\n",
      "\n",
      " Validation metric has not improved in last 55 epochs\n",
      "Epoch [69/150], Batch [200/200], Train Loss: 0.2088, Classification: 0.0114, FTR_ODE: 0.0009, RSK_ODE: 0.1711, RANKING: 0.0254\n",
      " CI: 0.6962525555614154  AUC: 0.7335712031118015\n",
      "\n",
      " Val Metric: 1.429823758673217  Last Epoch took 801.6636254787445 seconds\n",
      "\n",
      " Validation metric has not improved in last 56 epochs\n",
      "Epoch [70/150], Batch [200/200], Train Loss: 0.1827, Classification: 0.0046, FTR_ODE: 0.0012, RSK_ODE: 0.1533, RANKING: 0.0236\n",
      " CI: 0.7111275968223058  AUC: 0.7535334145242115\n",
      "\n",
      " Val Metric: 1.4646610113465173  Last Epoch took 800.5043942928314 seconds\n",
      "\n",
      " Validation metric has not improved in last 57 epochs\n",
      "Epoch [71/150], Batch [200/200], Train Loss: 0.2282, Classification: 0.0130, FTR_ODE: 0.0011, RSK_ODE: 0.1887, RANKING: 0.0254\n",
      " CI: 0.6895274347725051  AUC: 0.7198603123780274\n",
      "\n",
      " Val Metric: 1.4093877471505325  Last Epoch took 814.5664856433868 seconds\n",
      "\n",
      " Validation metric has not improved in last 58 epochs\n",
      "Epoch [72/150], Batch [200/200], Train Loss: 0.1860, Classification: 0.0121, FTR_ODE: 0.0009, RSK_ODE: 0.1494, RANKING: 0.0235\n",
      " CI: 0.7095315236022294  AUC: 0.7472073513949441\n",
      "\n",
      " Val Metric: 1.4567388749971735  Last Epoch took 806.746052980423 seconds\n",
      "\n",
      " Validation metric has not improved in last 59 epochs\n",
      "Epoch [73/150], Batch [200/200], Train Loss: 0.1746, Classification: 0.0228, FTR_ODE: 0.0008, RSK_ODE: 0.1283, RANKING: 0.0226\n",
      " CI: 0.6833834027905048  AUC: 0.7325045068837778\n",
      "\n",
      " Val Metric: 1.4158879096742827  Last Epoch took 810.414968252182 seconds\n",
      "\n",
      " Validation metric has not improved in last 60 epochs\n",
      "Epoch [74/150], Batch [200/200], Train Loss: 0.2400, Classification: 0.0211, FTR_ODE: 0.0011, RSK_ODE: 0.1917, RANKING: 0.0261\n",
      " CI: 0.6318386180946399  AUC: 0.6589114904872994\n",
      "\n",
      " Val Metric: 1.2907501085819393  Last Epoch took 823.5391545295715 seconds\n",
      "\n",
      " Validation metric has not improved in last 61 epochs\n",
      "Epoch [75/150], Batch [200/200], Train Loss: 0.1881, Classification: 0.0217, FTR_ODE: 0.0008, RSK_ODE: 0.1399, RANKING: 0.0257\n",
      " CI: 0.6604234782796702  AUC: 0.6837646146033476\n",
      "\n",
      " Val Metric: 1.3441880928830177  Last Epoch took 805.3798978328705 seconds\n",
      "\n",
      " Validation metric has not improved in last 62 epochs\n",
      "Epoch [76/150], Batch [200/200], Train Loss: 0.1567, Classification: 0.0091, FTR_ODE: 0.0008, RSK_ODE: 0.1255, RANKING: 0.0213\n",
      " CI: 0.6615249524358505  AUC: 0.6742160256646337\n",
      "\n",
      " Val Metric: 1.335740978100484  Last Epoch took 818.6389594078064 seconds\n",
      "\n",
      " Validation metric has not improved in last 63 epochs\n",
      "Epoch [77/150], Batch [200/200], Train Loss: 0.2189, Classification: 0.0029, FTR_ODE: 0.0012, RSK_ODE: 0.1922, RANKING: 0.0227\n",
      " CI: 0.673857718614165  AUC: 0.7014874807662768\n",
      "\n",
      " Val Metric: 1.3753451993804418  Last Epoch took 799.4590127468109 seconds\n",
      "\n",
      " Validation metric has not improved in last 64 epochs\n",
      "Epoch [78/150], Batch [200/200], Train Loss: 0.1508, Classification: 0.0057, FTR_ODE: 0.0011, RSK_ODE: 0.1231, RANKING: 0.0209\n",
      " CI: 0.6738318165339819  AUC: 0.710840074540253\n",
      "\n",
      " Val Metric: 1.3846718910742348  Last Epoch took 796.7061944007874 seconds\n",
      "\n",
      " Validation metric has not improved in last 65 epochs\n",
      "Epoch [79/150], Batch [50/200], Train Loss: 0.2299, Classification: 0.0413, FTR_ODE: 0.0010, RSK_ODE: 0.1653, RANKING: 0.0224\r"
     ]
    }
   ],
   "source": [
    "fld=1\n",
    "stp_sz=0.06 # Neural-ODE 36 month=1  so 1 mnth step_sz=0.03, 2 mnth=0.06, 3mnth=0.08, 4 mnth=0.10\n",
    "ode_solver='euler' # rk4     euler   midpoint\n",
    "\n",
    "\n",
    "############### Data Loader #######################################\n",
    "\n",
    "val_data=validation_dataset(fold=fld)\n",
    "val_loader=DataLoader(dataset=val_data, batch_size=16, shuffle=False, num_workers=2, \n",
    "                             pin_memory=False, drop_last=False, worker_init_fn=worker_init_fn)\n",
    "\n",
    "\n",
    "train_data=train_dataset(fold=fld, discard_converted=False)\n",
    "train_loader=DataLoader(dataset=train_data, batch_size=1, shuffle=True, num_workers=2, \n",
    "                             pin_memory=False, drop_last=False, worker_init_fn=worker_init_fn)\n",
    "\n",
    "\n",
    "\n",
    "################## Prepare the model  ############################### \n",
    "encoder_model=Encoder_Network2D()\n",
    "classifier_model=Classification_Network()\n",
    "ode_func=ODE_network(n_head=12, depth=3, ftr_dim=768)\n",
    "rank_model=Temporal_Order_Classification()\n",
    "\n",
    "encoder_model.to(device)\n",
    "classifier_model.to(device)\n",
    "ode_func.to(device)\n",
    "rank_model.to(device)\n",
    "\n",
    "#######################################################################\n",
    "train_complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    ".769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88fc30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
