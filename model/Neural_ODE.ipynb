{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a25a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange#, reduce, repeat\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffb3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Transformer_Block(nn.Module):\n",
    "    def __init__(self, n_head, ftr_dim):\n",
    "        super(Base_Transformer_Block, self).__init__()\n",
    "        \n",
    "        self.n_head=n_head\n",
    "        self.ftr_dim=ftr_dim\n",
    "        head_dim=ftr_dim//n_head\n",
    "        self.head_dim=head_dim\n",
    "        \n",
    "        # input x, h are un-normalized, can take any + or negative value\n",
    "        self.project_h=nn.Sequential(nn.GroupNorm(num_groups=1, num_channels=ftr_dim, affine=True),#layerNorm\n",
    "                                     nn.Softplus(),\n",
    "                                     nn.Linear(in_features=ftr_dim, out_features=ftr_dim, bias=False),\n",
    "                                     nn.GroupNorm(num_groups=1, num_channels=ftr_dim, affine=True),#layerNorm\n",
    "                                     nn.Softplus())    \n",
    "        \n",
    "        #########################################################################################################\n",
    "        \n",
    "        self.update_reset_lyr=nn.Sequential( # B,2.dim,n_head\n",
    "                                     nn.Conv1d(in_channels=(head_dim), out_channels=(2*head_dim),\n",
    "                                               kernel_size=1, stride=1, padding=0, groups=1, bias=False),\n",
    "                                     \n",
    "                                     nn.GroupNorm(num_groups=1, num_channels=(2*head_dim), affine=True),#layerNorm\n",
    "                                     nn.Sigmoid())\n",
    "                                    \n",
    "        \n",
    "        self.candidate_activation_vector=nn.Sequential( # B,2.dim,n_head\n",
    "                                         nn.Conv1d(in_channels=(head_dim), out_channels=(head_dim),\n",
    "                                               kernel_size=1, stride=1, padding=0, groups=1, bias=False),\n",
    "                                     \n",
    "                                         nn.GroupNorm(num_groups=1, num_channels=(head_dim), affine=True),#layerNorm\n",
    "                                         nn.Softplus())                                    \n",
    "        \n",
    "        ##########################################################################################################\n",
    "        \n",
    "        self.project_v=nn.Linear(in_features=ftr_dim, out_features=ftr_dim, bias=True)\n",
    "                                    # un-normalized, can take any + or - value\n",
    "        \n",
    "        ##########################################################################################################\n",
    "        \n",
    "        \n",
    "    def forward(self, h):\n",
    "        # h is the ftr at all depths.\n",
    "        # x_inp is h from the previous state.\n",
    "        \n",
    "        # Project the x and h to n_head sub-spaces each with 768//n_head dimensions\n",
    "        h=self.project_h(h) # [h^1, h^2, ... h^M]            \n",
    "        h=rearrange(h, 'b (d m) -> b d m', d=self.head_dim, m=self.n_head) # B,dim,n_head\n",
    "        \n",
    "        update_reset=self.update_reset_lyr(h) # sigmoid, multi-head with 1x1 conv\n",
    "        u=update_reset[:, 0:self.head_dim, :] # B,head_dim,n_head\n",
    "        r=update_reset[:, self.head_dim:,:]\n",
    "        \n",
    "        g=self.candidate_activation_vector((h*r)) # B, head_dim, n_head\n",
    "        \n",
    "        v=u*(g-h) # B,head_dim,n_head\n",
    "        \n",
    "        #### concatenate all heads and apply fc layer\n",
    "        v=rearrange(v, 'b d m -> b (d m)')\n",
    "        v=self.project_v(v)\n",
    "        ### residual connection\n",
    "        #v=v+x_inp\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2166fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be31178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Block(nn.Module):\n",
    "    def __init__(self, n_head, ftr_dim):\n",
    "        super(Transformer_Block, self).__init__()\n",
    "        \n",
    "        self.n_head=n_head\n",
    "        self.ftr_dim=ftr_dim\n",
    "        head_dim=ftr_dim//n_head\n",
    "        self.head_dim=head_dim\n",
    "        \n",
    "        # input x, h are un-normalized, can take any + or negative value\n",
    "        self.project_x=nn.Sequential(nn.GroupNorm(num_groups=1, num_channels=ftr_dim, affine=True),#layerNorm\n",
    "                                     nn.Softplus(),\n",
    "                                     nn.Linear(in_features=ftr_dim, out_features=ftr_dim, bias=False),\n",
    "                                     nn.GroupNorm(num_groups=1, num_channels=ftr_dim, affine=True),#layerNorm\n",
    "                                     nn.Softplus())\n",
    "                                    \n",
    "        \n",
    "        self.project_h=nn.Sequential(nn.GroupNorm(num_groups=1, num_channels=ftr_dim, affine=True),#layerNorm\n",
    "                                     nn.Softplus(),\n",
    "                                     nn.Linear(in_features=ftr_dim, out_features=ftr_dim, bias=False),\n",
    "                                     nn.GroupNorm(num_groups=1, num_channels=ftr_dim, affine=True),#layerNorm\n",
    "                                     nn.Softplus())    \n",
    "        \n",
    "        #########################################################################################################\n",
    "        \n",
    "        self.update_reset_lyr=nn.Sequential( # B,2.dim,n_head\n",
    "                                     nn.Conv1d(in_channels=(2*head_dim), out_channels=(2*head_dim),\n",
    "                                               kernel_size=1, stride=1, padding=0, groups=1, bias=False),\n",
    "                                     \n",
    "                                     nn.GroupNorm(num_groups=1, num_channels=(2*head_dim), affine=True),#layerNorm\n",
    "                                     nn.Sigmoid())\n",
    "                                    \n",
    "        \n",
    "        self.candidate_activation_vector=nn.Sequential( # B,2.dim,n_head\n",
    "                                         nn.Conv1d(in_channels=(2*head_dim), out_channels=(head_dim),\n",
    "                                               kernel_size=1, stride=1, padding=0, groups=1, bias=False),\n",
    "                                     \n",
    "                                         nn.GroupNorm(num_groups=1, num_channels=(head_dim), affine=True),#layerNorm\n",
    "                                         nn.Softplus())                                    \n",
    "        \n",
    "        ##########################################################################################################\n",
    "        \n",
    "        self.project_v=nn.Linear(in_features=ftr_dim, out_features=ftr_dim, bias=True)\n",
    "                                    # un-normalized, can take any + or - value\n",
    "        \n",
    "        ##########################################################################################################\n",
    "        \n",
    "        \n",
    "    def forward(self, h, x_inp):\n",
    "        # h is the ftr at all depths.\n",
    "        # x_inp is h from the previous state.\n",
    "        \n",
    "        # Project the x and h to n_head sub-spaces each with 768//n_head dimensions\n",
    "        x=self.project_x(x_inp) # [x^1, x^2, ... x^M]\n",
    "        h=self.project_h(h) # [h^1, h^2, ... h^M]\n",
    "        \n",
    "        x=rearrange(x, 'b (d m) -> b d m', d=self.head_dim, m=self.n_head) # B,dim,n_head\n",
    "        h=rearrange(h, 'b (d m) -> b d m', d=self.head_dim, m=self.n_head) # B,dim,n_head\n",
    "        \n",
    "        tmp_inp=torch.cat((x, h), dim=1) # B,(2*dim),n_head\n",
    "        update_reset=self.update_reset_lyr(tmp_inp) # sigmoid, multi-head with 1x1 conv\n",
    "        u=update_reset[:, 0:self.head_dim, :] # B,head_dim,n_head\n",
    "        r=update_reset[:, self.head_dim:,:]\n",
    "        \n",
    "        tmp_inp=torch.cat((x, (h*r)), dim=1) # B,(2*dim),n_head\n",
    "        g=self.candidate_activation_vector(tmp_inp) # B, head_dim, n_head\n",
    "        \n",
    "        v=u*(g-h) # B,head_dim,n_head\n",
    "        \n",
    "        #### concatenate all heads and apply fc layer\n",
    "        v=rearrange(v, 'b d m -> b (d m)')\n",
    "        v=self.project_v(v)\n",
    "        ### residual connection\n",
    "        v=v+x_inp\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b892ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODE_network(nn.Module):\n",
    "    def __init__(self, n_head=12, depth=3, ftr_dim=768):\n",
    "        super(ODE_network, self).__init__()                                  \n",
    "        self.base_ode_gru_lyr=Base_Transformer_Block(n_head, ftr_dim)\n",
    "        \n",
    "        lst=[]\n",
    "        for d in range(0, depth-1):\n",
    "            lst.append(Transformer_Block(n_head, ftr_dim))\n",
    "        \n",
    "        self.deep_ode_lyr=nn.ModuleList(lst)\n",
    "        self.depth=depth\n",
    "    \n",
    "    \n",
    "    def forward(self, t, h):\n",
    "        #h hidden state : same as feature embedding F\n",
    "        delta_h=self.base_ode_gru_lyr(h) # ftr \n",
    "        for d in range(0, self.depth-1):\n",
    "            delta_h=self.deep_ode_lyr[d](h, delta_h)\n",
    "        \n",
    "        return delta_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c72e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d982f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f444ffbd",
   "metadata": {},
   "source": [
    "ode_model=ODE_network(n_head=12, depth=3, ftr_dim=768)\n",
    "print(ode_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1365e729",
   "metadata": {},
   "source": [
    "ftr=np.ones((12, 768))\n",
    "ftr=torch.FloatTensor(ftr)\n",
    "t=torch.FloatTensor(np.array([0.1]))\n",
    "\n",
    "print(ftr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e995d1b",
   "metadata": {},
   "source": [
    "delta_h=ode_model(t, ftr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80d6aa",
   "metadata": {},
   "source": [
    "print(delta_h)\n",
    "print(delta_h.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
